apiVersion: v1
kind: Pod
metadata:
  name: rag-llm-ollama
  labels:
    k8s-app: rag-llm-ollama
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
            - key: nautilus.io/csu-tide
              operator: Exists
            - key: nvidia.com/gpu.product
              operator: In
              values:
              - NVIDIA-L40
  tolerations:
  - effect: NoSchedule
    key: nautilus.io/csu-tide
    operator: Exists
  containers:
  - name: rag-llm-ollama
    image: nvidia/cuda:12.4.0-devel-ubuntu22.04
    command: ["/bin/bash", "-c"]
    args:
    - >-
        cd /root;
        git clone https://github.com/SDSU-Research-CI/rag-llm-quick-start;
        mv rag-llm-quick-start code;
        cd code;
        chmod +x setup.sh;
        ./setup.sh;
        python app.py;
    resources:
      limits:
        nvidia.com/gpu: 1
        cpu: 2
        memory: 4Gi
      requests:
        nvidia.com/gpu: 1
        cpu: 2
        memory: 4Gi
    volumeMounts:
        - mountPath: /chroma
          name: rag-llm-chroma
  volumes:
  - name: rag-llm-chroma
    persistentVolumeClaim:
      claimName: rag-llm-chroma
