apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-llm-ollama
  labels:
    k8s-app: rag-llm-ollama
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: rag-llm-ollama
  template:
    metadata:
      labels:
        k8s-app: rag-llm-ollama 
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nautilus.io/csu-tide
                operator: Exists
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-L40
      tolerations:
      - effect: NoSchedule
        key: nautilus.io/csu-tide
        operator: Exists
      containers:
      - name: rag-llm-ollama
        image: kkricksdsu/ai-student-survey-chatbot:v0.4
        command: ["/bin/bash", "-c"]
        args:
        - >-
            systemctl daemon-reload;
            systemctl enable ollama;
            systemctl start ollama;
            sleep 60;
            ollama pull llama3;
            ollama pull nomic-embed-text;
            python3 app.py;
        resources:
          limits:
            nvidia.com/gpu: 1
            cpu: 2
            memory: 8Gi
          requests:
            nvidia.com/gpu: 1
            cpu: 2
            memory: 8Gi
        volumeMounts:
            - mountPath: /chroma
              name: rag-llm-chroma
        env:
          - name: WEB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: webpass
                key: password
      volumes:
      - name: rag-llm-chroma
        persistentVolumeClaim:
          claimName: rag-llm-chroma
